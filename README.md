
---

# ğŸš€ VisionAsk AI - TensorFlow Version

**Multimodal Vision-Language System**  
A pure TensorFlow implementation that combines **Object Detection**, **Image Captioning**, **Visual Question Answering (VQA)**, and **Sentiment Analysis** into one pipeline.

---

## âœ¨ Features
- ğŸ” **Object Detection** â€“ EfficientDet-D0 via TensorFlow Hub  
- ğŸ“ **Image Captioning** â€“ InceptionV3 feature extraction + rule-based captions  
- ğŸ¤– **Visual Question Answering (VQA)** â€“ Simple logic-based QA using detection results  
- ğŸ’¬ **Sentiment Analysis** â€“ Text sentiment classification for captions/questions (Hugging Face Transformers)  
- ğŸ¯ **Complete Analysis Pipeline** â€“ Runs detection, captioning, QA, and statistics in sequence

---

## ğŸ“¦ Installation
Run the following in **Google Colab** or your local environment:

```bash
pip install tensorflow tensorflow-hub opencv-python-headless
pip install pillow matplotlib numpy requests
pip install transformers
pip install tf-keras
```

---

## ğŸ–¼ï¸ Usage

### 1. Load Sample Images
```python
sample_images = load_diverse_images()
```

### 2. Object Detection
```python
detector = TensorFlowObjectDetector()
detections = detector.detect(sample_images['beach'], threshold=0.4)
detector.visualize(sample_images['beach'], detections)
```

### 3. Image Captioning
```python
captioner = TensorFlowImageCaptioner()
caption = captioner.generate_caption(sample_images['food'])
print("Caption:", caption)
```

### 4. Visual Question Answering
```python
vqa = TensorFlowVQA(detector)
answer = vqa.answer_question(sample_images['animals'], "How many animals are there?")
print("Answer:", answer)
```

### 5. Complete Analysis Pipeline
```python
analyze_image_complete(sample_images['city'], 'city')
```

---

## ğŸ“Š Example Output
- **Object Detection**: â€œFound 3 objects: person (92%), car (85%), dog (78%)â€  
- **Caption**: â€œA photo featuring a person, a car, and a dogâ€  
- **VQA**:  
  - Q: *Is there a person in the image?*  
    A: *Yes, there is a person*  
- **Statistics**: `{ 'person': 2, 'car': 1, 'dog': 1 }`

---

## ğŸ› ï¸ Project Structure
```
VisionAskAI/
â”‚â”€â”€ visionask_ai.py        # Main pipeline
â”‚â”€â”€ README.md              # Project documentation
â”‚â”€â”€ requirements.txt       # Dependencies
```

---

## ğŸš€ Future Improvements
- Integrate pretrained captioning models (BLIP, ViT-GPT2)  
- Advanced VQA with multimodal transformers (VisualBERT, LXMERT)  
- Interactive demo using **Gradio** or **Streamlit**  
